# Project : Data Warehousing with AWS Redshift

![Python3](https://img.shields.io/badge/python3-%20-blue) 
![SQL](https://img.shields.io/badge/SQL-%20-yellowgreen) 
![Redshift](https://img.shields.io/badge/Redshift-%20-red) 
![DataWarehousing](https://img.shields.io/badge/Data-%20Warehousing-orange) 
![Udacityproject](https://img.shields.io/badge/Udacity-Project-blue) 
![DataEngineering](https://img.shields.io/badge/Data-Engineering-green)
![ETL](https://img.shields.io/badge/ETL-%20-brightgreen)

![AWS](https://img.shields.io/badge/Amazon_AWS-FF9900?style=for-the-badge&logo=amazonaws&logoColor=white)
![GIT](https://img.shields.io/badge/GIT-E44C30?style=for-the-badge&logo=git&logoColor=white)

<br> <br>

<!-- ABOUT THE PROJECT -->
## About The Project

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They would like a data engineer to build an ETL pipeline that extract their data from S3, stage them in Redshift, and transform data into a set of dimensional tables for their Analytics team to continue finding insights into what songs their users are listening to. The objective of the project is to create a data warehouse on cloud (AWS Redshift) and build the ETL pipeline to prepare the data for the Analytics team. 

<br>

### Project Description :

In this project, the data engineer is required to build an ETL pipeline for a database hosted on Redshift. To complete the project, the data needs to be loaded from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.



### Project Datasets :
We will be working with 2 datasets (Song Data & Log Data) in this project, that resides as json files in AWS S3.

#### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

Below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

#### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. 

### Database Schema

### Project Workflow

<br><br>


### Project Structure
<div align=center><br>

|     File / Folder      |                         Description                          |
| :--------------------: | :----------------------------------------------------------: |
|         static_resources         |  Folder at the root of the project, where static resources/images are present  |
|     sql_queries.py     | Contains the SQL queries for staging, schema definition and ETL |
|    create_tables.py    | Drops and creates tables on AWS Redshift (Reset the tables)  |
|         etl.py         | Stages and transforms the data from S3 buckets and loads them into tables |
|        dwh.cfg         |              Sample configuration file for AWS               |
|         README         |                         Readme file                          |

<br></div>

#### Steps to run the scripts

<!-- Connect with me -->
<div align=center>
<br><br><br>

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/deepankar-acharyya-034053a5/)

[![Github](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/DeepankarAcharyya)

<br><br>
</div>


